\chapter{Conclusion}\label{sec-conclusion}

Explainability is a critical but often-overlooked property for intelligent systems. In this survey, we review techniques that provide explainability to classifiers, with a special focus on visualization. We first discuss the concept and definition of explainability of a classifier. Two types of techniques that provide explainability to classifiers are summarized and discussed. Then based on the life cycle of intelligent systems, we discuss the role of visualization in different stages, and how visualization can be used to improve explainability.

The research in the explainability of classifiers is still a growing discipline. There is no consensus on the definition of explainability in the context of supervised learning. There is few rigorous evaluation methods of the explainability of a classifier or a explanation of a classifier. Early work treats the explainability as the ``simplicity'' and always focuses on balancing the trade-offs between performance and simplicity. Now there is a new trend of building an explanatory interface between human and the underlying model to enhance explainability while maintaining the performance. A few challenges and research issues are summarized as follows.

\textbf{Rigorous theory of explainability}. There are plenty of unsolved questions in the the seemly intuitive concept of explainability. How to rigorously define explainability in the context of machine learning or artificial intelligence? What is explanation? How to evaluate whether an explanation is good or not? How can we model the variation and uncertainty of human in the explanatory interface? Based on the theory of explainability, a further issue that needs to address is the \textbf{evaluation} of explainability and the quality of explanations. If a metric based evaluation is inapplicable, there still need guidelines for system designs. There is some work in cognitive science studying the function, structure of explanation, and the role that explanation plays in perception, cognition, and learning. It may be promising to learn from the theories from cognition science \cite{ritter2017cognitive}. Still, it requires the efforts from both cognitive science and computer science.

\textbf{Applications}. Theory comes from practice. Although there are toy examples, showing how explainability helps design better models, avoid possible bias, and enhance users' trust, we still lack knowledge on the design challenges of a explainable interface in real-world applications. Is it possible to let the machine explain its learned knowledge to a human? There are few successes or failures in real-world applications that we can learn from. Another neglected stakeholder is the end-users who actually use or are influenced by the technology developed based on AI. How do they value explainability during the use of an intelligent system? How can we use explanatory techniques to improve their using experience? The research at this end might be able to impact more people.

% \textbf{}


% \section{Other Applications}
% \subsection{Teaching and Communicating Models}
% \cite{harley2015isvc}
% Basilio Noris develop a visualization tool, MLDemos\footnote{\url{http://mldemos.epfl.ch/}}, for understanding how different algorithms and parameters influence the results in different machine learning problems. The tool project the decision space back to the input space,  how the output space of a classifier is 
% \cite{wongsuphasawat2017dataflow}
% Narrative, Interactive, etc. to explain your model to others.
% \subsection{Learn from the Model} 
% Knowledge Discovery; Learn lessons from what the model learned (Alpha Go)