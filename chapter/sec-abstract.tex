\begin{abstract}

Classification is a fundamental problem in machine learning, data mining and computer vision. In practice, interpretability is a desired property of classification models (classifiers) in critical areas like security, medicine and finance. For instance, a quantitative trader may prefer a more interpretable model with less expected return due to its predictability. Unfortunately, most best-performing classifiers in many applications (e.g., deep neural networks) are complex machines whose predictions are difficult to explain. Thus, there is a growing interest in using visualization to understand, diagnose and explain machine learning systems in both academia and industry. Many challenges need to be addressed in the formalization of explainability and design principles and evaluation of explainable intelligent systems.

The survey starts with an introduction on the concept and background of explainable classifiers. Existing work in both visualization and machine learning communities is categorized in terms of data types and purposes of explanation. Then the survey ends with a discussion on the challenges and future research opportunities of explainable classifiers.

\end{abstract}
